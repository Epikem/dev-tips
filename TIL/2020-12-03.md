# TIL 2020-12-03

--------------------------

## 할 일

- [ ] margin 작업
- [x] 알고리즘 문제풀이
- [ ] 지원준비, 서류쓰기
- [x] 개인 일정관리 도입에 대한 고민
- [x] 변재영 멘토님 쿠버네티스 멘토링

## margin 작업

- [ ] config 생성
- [ ] sequelize 적용
- [ ] git-secret 적용
- [ ] express 적용
- [ ] express-lambda 적용


## 알고리즘 문제풀이

### 백준 [2573 - 빙산](https://www.acmicpc.net/problem/2573)

각 칸에 들어가는 수는 10 이하이고, 수가 들어가는 칸은 1만개 이하이므로, 모든 칸의 모든 수를 센다고 해도 10만 번 이하의 연산 횟수를 가진다.

그러나, 단순 세는 것이 아니라 계속 변하는 주변의 0의 개수에 따라 녹는 속도가 변하는 것을 구현해야 하고, 두 덩어리 이상으로 분리되었는지 아닌지를 확인해야 하기 때문에, 이를 어떻게 구현해야 할 지 생각이 필요하다.

매 번 모든 칸의 주변 4칸을 확인한다면, 연산량은 대략 1만 * 10 * 4 = 40만 정도

그런데, 빙산이 분리되었는지는 어떻게 검사할 수 있을까??
가로세로로 읽어나가면서, 인접한 칸들을 모두 같은 숫자로 지정해놓는 맵으로 관리? 효율적일까??

빙산 칸의 개수가 1만개 이하이므로, 모든 새 빙산에 대해 수를 할당한 후, 인접한 빙산들은 같은 그룹으로 할당시키는 방식으로 구현?

역시 시간 초과가 난다.

그렇다면 녹이는 것, 또는 그룹을 구하는 과정을 좀더 빠르게 만들 방법을 찾아야 한다.

그러면, 현재 구현에서 녹이는 과정과 그룹 구하는 과정의 구현을 분석하여 속도를 추정해 보자.

녹이는 과정:
1. 맵을 복사 (맵크기)
2. 모든 칸에 대해 (맵크기)
   1. 주변의 물 개수를 센다 (x4)
   2. 새 맵에 (이전 빙산높이 - 주변 물 개수)로 할당

약 `2 x (맵크기)`

그룹 구하는 과정:
1. 새 방문 여부 맵 생성 (맵크기)
2. 모든 칸에 대해: (맵크기)
   1. 고유 그룹 번호 배정
3. 모든 칸에 대해: (맵크기)
   1. dfs로 인접한 그룹에 대해 같은 번호 배정
4. 모든 칸에 대해: (맵크기)
   1. 그룹 번호를 set에 추가

약 `4 x (맵크기)`

이 과정을 빙하가 녹을 때까지 반복한다.
문제는, 한 숫자가 최대 10이라지만, 속에 있는 빙하는 주변 물이 다 녹아야 녹기 때문에, 10만 이상의 시간이 걸릴 수 있다는 점이다.

맵크기 9만 * 10만 정도면, 이미 1억을 넘었다.

그렇다면, 녹이는 과정 자체를 스킵하는 방법을 찾지 않는 이상, 녹이는 과정과 그룹 구하는 과정 모두를 효율적으로 만들어야만 가능하다는 얘기다.

가능하냐?

일단 논리로 생각해 보자.

### 녹이는 과정:
- 녹이는 과정에서, 다음 맵을 새로 만들지 않고 이전에 만든 것을 초기화해서 쓰면 더 빠를까?
- 1만개라고 했을 때, 10000=100*100 이고, 길이 절반은 50이므로 가운데 까지 녹으려면 50 * 10 이어야 할 듯하다.

1. 녹이기 전용 주변 물 개수 맵 생성.
2. 녹이는 과정에서 새 맵을 생성하지 않고 주변 물개수 맵

### 그룹 구하는 과정:

처음에 그룹을 다 계산한 다음, 녹일 때마다 녹은 곳을 알아내고, 이 영향으로 그룹을 빠르게 계산할 수 있어야 한다.


</details>

## 개인 일정관리 도입에 대한 고민

캘린더 통합 작업중에 있음.



## 변재영 멘토님 쿠버네티스 멘토링

pod나 node에 라벨링,
쿠버네티스는 마스터에 pod 배포할 수 없다

annotation: pod에 주석 달기

echo $? : 이전 명령 종료 코드 출력

liveness-probe

kubernetes controllers


resource ~= object

controllers
replication
replicaSet
deployments의 하위 요소
replication의 새 버전
deployment
롤링 업데이트, 배포 중단, 롤백 가능
deamonSet
deployment 기능 포함
한개 pod가 모든 노드에 실행되도록 함
deployments와는 다르게 로그 수집 등 비중요 서비스이므로 변경시 인스턴스를 먼저 끄고 다음 서비스를 켠다
job
activeDeadlineSeconds 전체 작업 제한 시간, 넘으면 바로 실패
completion
parallelism
restartPolicy: onFailure/Never
onFailure해도 activeDeadlineSeconds가 우선한다.



스핀에이커? 카나리 배포를 지원
카카오에서도 사용

실제 b2c 서비스에서는 job이 엄청 많다. (로그지우기, 통계 모으기, 기타등등)
job이 중복되면 리스크가 크다 (리소스를 많이 먹기 때문)
따라서 job은 보통 많이 돌지 않는 것이 좋다.



rc pod template을 변경하면 이후 생성되는 pod에만 적용된다
(재생성 해야 한다)


kubectl commands:
edit : 수정
create : 생성, 겹치면 실패
apply : 생성 후 수정, 겹치면 경고만
scale
replace







## tags
- \#TIL, \#blog, \#prj, \#algorithm, \#sport

--------------------------