# TIL 2020-11-28

--------------------------

## 할 일

- [x] 과제하기
- [ ] 지원준비, 서류쓰기
- [ ] 알고리즘 문제풀이
- [ ] 논문(요약)쓰기
- [x] 김수현 멘토님 멘토링 - 딥러닝 경량화
- [ ] margin - 작업

## 논문(요약)쓰기



## 김수현 멘토님 멘토링 - 딥러닝 경량화

자연어 처리에 GPT, BERT가 대세

성능이 좋지만, 모델이 크고 무거움.


딥러닝 모델 자체의 경량화 방법에 대해 논의 (하드웨어 단에 대해서는 논외 : simd, 병렬화, gpu 등)
각각의 개념에 대해 소개.

- network pruning
신경 밀도가 태어날때 적고, 6살에 많다가, 14살에는 다시 적음
adhd와도 관계가 있지 않을까? (쓸모없는 노드에 정신 소모)

아이러니: 노드 가지치기가 아닌 weight pruning을 해 봐야, gpu는 행렬곱에 최적화되어있기 때문에 네트워크 구조가 이상해져서 성능 이득이 크지 않다. 그래서 노드 자체를 자르는  structured pruning을 한다. 
(structured pruning을 다르게는 안되나..?)

- pruning 조건
필터 = 채널


- knowledge distillation

- parameter quantization


- architecture design
- dynamic computation




## tags
- \#TIL, \#blog, \#prj, \#ml

--------------------------